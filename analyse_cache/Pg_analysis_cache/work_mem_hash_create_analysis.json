{
  "param_name": "work_mem",
  "function_name": "hash_create",
  "analysis": "<需要提供函数>\n目前提供的函数片段已经足够分析，不需要补充其他函数的具体实现。\n\n<思考过程>\n1. 参数通过控制关键函数对数据库性能的影响：\n    - `work_mem` 通过内存配置机制影响 `hash_create`，进而影响哈希表的初始化和分配。\n    - 机制：`work_mem` 指定了单个操作内部排序或哈希表所允许使用的最大内存，这影响 `hash_create` 中用于哈希表或目录结构的内存。增加 `work_mem` 值将允许在 `hash_create` 中在内存中处理更多数据，无需溢写到磁盘。\n    - 数据库性能影响：较高的 `work_mem` 可以减少磁盘 I/O，因为可以在内存中建立更大的哈希表，减少磁盘写操作，提高查询速度。但过大的 `work_mem` 可能导致内存消耗过多，影响其他并发查询的性能。\n\n<火焰图采样分析与调优方向>\n2. 基于 `hash_create`执行状态和相关函数片段给出`work_mem` 优化建议：\n    - 如果涉及到其他函数，请指出除了监控`hash_create`之外，是否还需监控其他函数：没有需要监控的其他函数，`hash_create` 是主要评估目标。\n    - 如何根据 `hash_create`的火焰图采样率，推荐`work_mem`调整的方向（升高或降低）和依据：\n        - 如果 `hash_create` 的火焰图显示高频繁的内存分配失败或频繁的磁盘 I/O，则建议增加 `work_mem`，以便哈希表能够在内存中保持更多的数据。\n        - 如果 `hash_create` 的火焰图显示过度的内存使用影响其他系统函数或降低系统并发处理能力，则建议根据系统资源调整，降低`work_mem`以防止内存占用过多导致性能瓶颈。",
  "code_snippets": "hash_create(const char *tabname, long nelem, const HASHCTL *info, int flags)\n{\n\tHTAB\t   *hashp;\n\tHASHHDR    *hctl;\n\n\t/*\n\t * Hash tables now allocate space for key and data, but you have to say\n\t * how much space to allocate.\n\t */\n\tAssert(flags & HASH_ELEM);\n\tAssert(info->keysize > 0);\n\tAssert(info->entrysize >= info->keysize);\n\n\t/*\n\t * For shared hash tables, we have a local hash header (HTAB struct) that\n\t * we allocate in TopMemoryContext; all else is in shared memory.\n\t *\n\t * For non-shared hash tables, everything including the hash header is in\n\t * a memory context created specially for the hash table --- this makes\n\t * hash_destroy very simple.  The memory context is made a child of either\n\t * a context specified by the caller, or TopMemoryContext if nothing is\n\t * specified.\n\t */\n\tif (flags & HASH_SHARED_MEM)\n\t{\n\t\t/* Set up to allocate the hash header */\n\t\tCurrentDynaHashCxt = TopMemoryContext;\n\t}\n\telse\n\t{\n\t\t/* Create the hash table's private memory context */\n\t\tif (flags & HASH_CONTEXT)\n\t\t\tCurrentDynaHashCxt = info->hcxt;\n\t\telse\n\t\t\tCurrentDynaHashCxt = TopMemoryContext;\n\t\tCurrentDynaHashCxt = AllocSetContextCreate(CurrentDynaHashCxt,\n\t\t\t\t\t\t\t\t\t\t\t\t   \"dynahash\",\n\t\t\t\t\t\t\t\t\t\t\t\t   ALLOCSET_DEFAULT_SIZES);\n\t}\n\n\t/* Initialize the hash header, plus a copy of the table name */\n\thashp = (HTAB *) DynaHashAlloc(sizeof(HTAB) + strlen(tabname) + 1);\n\tMemSet(hashp, 0, sizeof(HTAB));\n\n\thashp->tabname = (char *) (hashp + 1);\n\tstrcpy(hashp->tabname, tabname);\n\n\t/* If we have a private context, label it with hashtable's name */\n\tif (!(flags & HASH_SHARED_MEM))\n\t\tMemoryContextSetIdentifier(CurrentDynaHashCxt, hashp->tabname);\n\n\t/*\n\t * Select the appropriate hash function (see comments at head of file).\n\t */\n\tif (flags & HASH_FUNCTION)\n\t{\n\t\tAssert(!(flags & (HASH_BLOBS | HASH_STRINGS)));\n\t\thashp->hash = info->hash;\n\t}\n\telse if (flags & HASH_BLOBS)\n\t{\n\t\tAssert(!(flags & HASH_STRINGS));\n\t\t/* We can optimize hashing for common key sizes */\n\t\tif (info->keysize == sizeof(uint32))\n\t\t\thashp->hash = uint32_hash;\n\t\telse\n\t\t\thashp->hash = tag_hash;\n\t}\n\telse\n\t{\n\t\t/*\n\t\t * string_hash used to be considered the default hash method, and in a\n\t\t * non-assert build it effectively still is.  But we now consider it\n\t\t * an assertion error to not say HASH_STRINGS explicitly.  To help\n\t\t * catch mistaken usage of HASH_STRINGS, we also insist on a\n\t\t * reasonably long string length: if the keysize is only 4 or 8 bytes,\n\t\t * it's almost certainly an integer or pointer not a string.\n\t\t */\n\t\tAssert(flags & HASH_STRINGS);\n\t\tAssert(info->keysize > 8);\n\n\t\thashp->hash = string_hash;\n\t}\n\n\t/*\n\t * If you don't specify a match function, it defaults to string_compare if\n\t * you used string_hash, and to memcmp otherwise.\n\t *\n\t * Note: explicitly specifying string_hash is deprecated, because this\n\t * might not work for callers in loadable modules on some platforms due to\n\t * referencing a trampoline instead of the string_hash function proper.\n\t * Specify HASH_STRINGS instead.\n\t */\n\tif (flags & HASH_COMPARE)\n\t\thashp->match = info->match;\n\telse if (hashp->hash == string_hash)\n\t\thashp->match = (HashCompareFunc) string_compare;\n\telse\n\t\thashp->match = memcmp;\n\n\t/*\n\t * Similarly, the key-copying function defaults to strlcpy or memcpy.\n\t */\n\tif (flags & HASH_KEYCOPY)\n\t\thashp->keycopy = info->keycopy;\n\telse if (hashp->hash == string_hash)\n\t{\n\t\t/*\n\t\t * The signature of keycopy is meant for memcpy(), which returns\n\t\t * void*, but strlcpy() returns size_t.  Since we never use the return\n\t\t * value of keycopy, and size_t is pretty much always the same size as\n\t\t * void *, this should be safe.  The extra cast in the middle is to\n\t\t * avoid warnings from -Wcast-function-type.\n\t\t */\n\t\thashp->keycopy = (HashCopyFunc) (pg_funcptr_t) strlcpy;\n\t}\n\telse\n\t\thashp->keycopy = memcpy;\n\n\t/* And select the entry allocation function, too. */\n\tif (flags & HASH_ALLOC)\n\t\thashp->alloc = info->alloc;\n\telse\n\t\thashp->alloc = DynaHashAlloc;\n\n\tif (flags & HASH_SHARED_MEM)\n\t{\n\t\t/*\n\t\t * ctl structure and directory are preallocated for shared memory\n\t\t * tables.  Note that HASH_DIRSIZE and HASH_ALLOC had better be set as\n\t\t * well.\n\t\t */\n\t\thashp->hctl = info->hctl;\n\t\thashp->dir = (HASHSEGMENT *) (((char *) info->hctl) + sizeof(HASHHDR));\n\t\thashp->hcxt = NULL;\n\t\thashp->isshared = true;\n\n\t\t/* hash table already exists, we're just attaching to it */\n\t\tif (flags & HASH_ATTACH)\n\t\t{\n\t\t\t/* make local copies of some heavily-used values */\n\t\t\thctl = hashp->hctl;\n\t\t\thashp->keysize = hctl->keysize;\n\t\t\thashp->ssize = hctl->ssize;\n\t\t\thashp->sshift = hctl->sshift;\n\n\t\t\treturn hashp;\n\t\t}\n\t}\n\telse\n\t{\n\t\t/* setup hash table defaults */\n\t\thashp->hctl = NULL;\n\t\thashp->dir = NULL;\n\t\thashp->hcxt = CurrentDynaHashCxt;\n\t\thashp->isshared = false;\n\t}\n\n\tif (!hashp->hctl)\n\t{\n\t\thashp->hctl = (HASHHDR *) hashp->alloc(sizeof(HASHHDR));\n\t\tif (!hashp->hctl)\n\t\t\tereport(ERROR,\n\t\t\t\t\t(errcode(ERRCODE_OUT_OF_MEMORY),\n\t\t\t\t\t errmsg(\"out of memory\")));\n\t}\n\n\thashp->frozen = false;\n\n\thdefault(hashp);\n\n\thctl = hashp->hctl;\n\n\tif (flags & HASH_PARTITION)\n\t{\n\t\t/* Doesn't make sense to partition a local hash table */\n\t\tAssert(flags & HASH_SHARED_MEM);\n\n\t\t/*\n\t\t * The number of partitions had better be a power of 2. Also, it must\n\t\t * be less than INT_MAX (see init_htab()), so call the int version of\n\t\t * next_pow2.\n\t\t */\n\t\tAssert(info->num_partitions == next_pow2_int(info->num_partitions));\n\n\t\thctl->num_partitions = info->num_partitions;\n\t}\n\n\tif (flags & HASH_SEGMENT)\n\t{\n\t\thctl->ssize = info->ssize;\n\t\thctl->sshift = my_log2(info->ssize);\n\t\t/* ssize had better be a power of 2 */\n\t\tAssert(hctl->ssize == (1L << hctl->sshift));\n\t}\n\n\t/*\n\t * SHM hash tables have fixed directory size passed by the caller.\n\t */\n\tif (flags & HASH_DIRSIZE)\n\t{\n\t\thctl->max_dsize = info->max_dsize;\n\t\thctl->dsize = info->dsize;\n\t}\n\n\t/* remember the entry sizes, too */\n\thctl->keysize = info->keysize;\n\thctl->entrysize = info->entrysize;\n\n\t/* make local copies of heavily-used constant fields */\n\thashp->keysize = hctl->keysize;\n\thashp->ssize = hctl->ssize;\n\thashp->sshift = hctl->sshift;\n\n\t/* Build the hash directory structure */\n\tif (!init_htab(hashp, nelem))\n\t\telog(ERROR, \"failed to initialize hash table \\\"%s\\\"\", hashp->tabname);\n\n\t/*\n\t * For a shared hash table, preallocate the requested number of elements.\n\t * This reduces problems with run-time out-of-shared-memory conditions.\n\t *\n\t * For a non-shared hash table, preallocate the requested number of\n\t * elements if it's less than our chosen nelem_alloc.  This avoids wasting\n\t * space if the caller correctly estimates a small table size.\n\t */\n\tif ((flags & HASH_SHARED_MEM) ||\n\t\tnelem < hctl->nelem_alloc)\n\t{\n\t\tint\t\t\ti,\n\t\t\t\t\tfreelist_partitions,\n\t\t\t\t\tnelem_alloc,\n\t\t\t\t\tnelem_alloc_first;\n\n\t\t/*\n\t\t * If hash table is partitioned, give each freelist an equal share of\n\t\t * the initial allocation.  Otherwise only freeList[0] is used.\n\t\t */\n\t\tif (IS_PARTITIONED(hashp->hctl))\n\t\t\tfreelist_partitions = NUM_FREELISTS;\n\t\telse\n\t\t\tfreelist_partitions = 1;\n\n\t\tnelem_alloc = nelem / freelist_partitions;\n\t\tif (nelem_alloc <= 0)\n\t\t\tnelem_alloc = 1;\n\n\t\t/*\n\t\t * Make sure we'll allocate all the requested elements; freeList[0]\n\t\t * gets the excess if the request isn't divisible by NUM_FREELISTS.\n\t\t */\n\t\tif (nelem_alloc * freelist_partitions < nelem)\n\t\t\tnelem_alloc_first =\n\t\t\t\tnelem - nelem_alloc * (freelist_partitions - 1);\n\t\telse\n\t\t\tnelem_alloc_first = nelem_alloc;\n\n\t\tfor (i = 0; i < freelist_partitions; i++)\n\t\t{\n\t\t\tint\t\t\ttemp = (i == 0) ? nelem_alloc_first : nelem_alloc;\n\n\t\t\tif (!element_alloc(hashp, temp, i))\n\t\t\t\tereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_OUT_OF_MEMORY),\n\t\t\t\t\t\t errmsg(\"out of memory\")));\n\t\t}\n\t}\n\n\tif (flags & HASH_FIXED_SIZE)\n\t\thashp->isfixed = true;\n\treturn hashp;\n}",
  "timestamp": "2025-06-04T19:27:36.061249"
}