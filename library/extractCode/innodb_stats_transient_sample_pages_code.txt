-------------------------------------------------------------------------------------------
File: /root/LLVM/mysql-8.0.36/storage/innobase/btr/btr0cur.cc
Function: btr_estimate_number_of_different_key_vals
bool btr_estimate_number_of_different_key_vals(
    dict_index_t *index) /*!< in: index */
{
  btr_cur_t cursor;
  page_t *page;
  rec_t *rec;
  ulint n_cols;
  uint64_t *n_diff;
  uint64_t *n_not_null;
  bool stats_null_not_equal;
  uintmax_t n_sample_pages; /* number of pages to sample */
  ulint not_empty_flag = 0;
  ulint total_external_size = 0;
  ulint i;
  ulint j;
  uintmax_t add_on;
  mtr_t mtr;
  mem_heap_t *heap = nullptr;
  ulint *offsets_rec = nullptr;
  ulint *offsets_next_rec = nullptr;

  /* For spatial index, there is no such stats can be
  fetched. */
  if (dict_index_is_spatial(index)) {
    return (false);
  }

  n_cols = dict_index_get_n_unique(index);

  heap =
      mem_heap_create((sizeof *n_diff + sizeof *n_not_null) * n_cols +
                          dict_index_get_n_fields(index) *
                              (sizeof *offsets_rec + sizeof *offsets_next_rec),
                      UT_LOCATION_HERE);

  n_diff = (uint64_t *)mem_heap_zalloc(heap, n_cols * sizeof(n_diff[0]));

  n_not_null = nullptr;

  /* Check srv_innodb_stats_method setting, and decide whether we
  need to record non-null value and also decide if NULL is
  considered equal (by setting stats_null_not_equal value) */
  switch (srv_innodb_stats_method) {
    case SRV_STATS_NULLS_IGNORED:
      n_not_null =
          (uint64_t *)mem_heap_zalloc(heap, n_cols * sizeof *n_not_null);
      [[fallthrough]];

    case SRV_STATS_NULLS_UNEQUAL:
      /* for both SRV_STATS_NULLS_IGNORED and SRV_STATS_NULLS_UNEQUAL
      case, we will treat NULLs as unequal value */
      stats_null_not_equal = true;
      break;

    case SRV_STATS_NULLS_EQUAL:
      stats_null_not_equal = false;
      break;

    default:
      ut_error;
  }

  /* It makes no sense to test more pages than are contained
  in the index, thus we lower the number if it is too high */
  if (srv_stats_transient_sample_pages > index->stat_index_size) {
    if (index->stat_index_size > 0) {
      n_sample_pages = index->stat_index_size;
    } else {
      n_sample_pages = 1;
    }
  } else {
    n_sample_pages = srv_stats_transient_sample_pages;
  }

  /* We sample some pages in the index to get an estimate */

  for (i = 0; i < n_sample_pages; i++) {
    mtr_start(&mtr);

    bool available;

    available = btr_cur_open_at_rnd_pos(index, BTR_SEARCH_LEAF, &cursor,
                                        __FILE__, __LINE__, &mtr);

    if (!available) {
      mtr_commit(&mtr);
      mem_heap_free(heap);

      return (false);
    }

    /* Count the number of different key values for each prefix of
    the key on this index page. If the prefix does not determine
    the index record uniquely in the B-tree, then we subtract one
    because otherwise our algorithm would give a wrong estimate
    for an index where there is just one key value. */

    page = btr_cur_get_page(&cursor);

    rec = page_rec_get_next(page_get_infimum_rec(page));

    if (!page_rec_is_supremum(rec)) {
      not_empty_flag = 1;
      offsets_rec = rec_get_offsets(rec, index, offsets_rec, ULINT_UNDEFINED,
                                    UT_LOCATION_HERE, &heap);

      if (n_not_null != nullptr) {
        btr_record_not_null_field_in_rec(index, n_cols, offsets_rec,
                                         n_not_null);
      }
    }

    while (!page_rec_is_supremum(rec)) {
      ulint matched_fields;
      rec_t *next_rec = page_rec_get_next(rec);
      if (page_rec_is_supremum(next_rec)) {
        total_external_size +=
            lob::btr_rec_get_externally_stored_len(index, rec, offsets_rec);
        break;
      }

      offsets_next_rec =
          rec_get_offsets(next_rec, index, offsets_next_rec, ULINT_UNDEFINED,
                          UT_LOCATION_HERE, &heap);

      cmp_rec_rec_with_match(rec, next_rec, offsets_rec, offsets_next_rec,
                             index, page_is_spatial_non_leaf(next_rec, index),
                             stats_null_not_equal, &matched_fields);

      for (j = matched_fields; j < n_cols; j++) {
        /* We add one if this index record has
        a different prefix from the previous */

        n_diff[j]++;
      }

      if (n_not_null != nullptr) {
        btr_record_not_null_field_in_rec(index, n_cols, offsets_next_rec,
                                         n_not_null);
      }

      total_external_size +=
          lob::btr_rec_get_externally_stored_len(index, rec, offsets_rec);

      rec = next_rec;
      /* Initialize offsets_rec for the next round
      and assign the old offsets_rec buffer to
      offsets_next_rec. */
      {
        ulint *offsets_tmp = offsets_rec;
        offsets_rec = offsets_next_rec;
        offsets_next_rec = offsets_tmp;
      }
    }

    if (n_cols == dict_index_get_n_unique_in_tree(index)) {
      /* If there is more than one leaf page in the tree,
      we add one because we know that the first record
      on the page certainly had a different prefix than the
      last record on the previous index page in the
      alphabetical order. Before this fix, if there was
      just one big record on each clustered index page, the
      algorithm grossly underestimated the number of rows
      in the table. */

      if (btr_page_get_prev(page, &mtr) != FIL_NULL ||
          btr_page_get_next(page, &mtr) != FIL_NULL) {
        n_diff[n_cols - 1]++;
      }
    }

    mtr_commit(&mtr);
  }

  /* If we saw k borders between different key values on
  n_sample_pages leaf pages, we can estimate how many
  there will be in index->stat_n_leaf_pages */

  /* We must take into account that our sample actually represents
  also the pages used for external storage of fields (those pages are
  included in index->stat_n_leaf_pages) */

  for (j = 0; j < n_cols; j++) {
    index->stat_n_diff_key_vals[j] = BTR_TABLE_STATS_FROM_SAMPLE(
        n_diff[j], index, n_sample_pages, total_external_size, not_empty_flag);

    /* If the tree is small, smaller than
    10 * n_sample_pages + total_external_size, then
    the above estimate is ok. For bigger trees it is common that we
    do not see any borders between key values in the few pages
    we pick. But still there may be n_sample_pages
    different key values, or even more. Let us try to approximate
    that: */

    add_on = index->stat_n_leaf_pages /
             (10 * (n_sample_pages + total_external_size));

    if (add_on > n_sample_pages) {
      add_on = n_sample_pages;
    }

    index->stat_n_diff_key_vals[j] += add_on;

    index->stat_n_sample_sizes[j] = n_sample_pages;

    /* Update the stat_n_non_null_key_vals[] with our
    sampled result. stat_n_non_null_key_vals[] is created
    and initialized to zero in dict_index_add_to_cache(),
    along with stat_n_diff_key_vals[] array */
    if (n_not_null != nullptr) {
      index->stat_n_non_null_key_vals[j] =
          BTR_TABLE_STATS_FROM_SAMPLE(n_not_null[j], index, n_sample_pages,
                                      total_external_size, not_empty_flag);
    }
  }

  mem_heap_free(heap);

  return (true);
}


